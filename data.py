# -*- coding: utf-8 -*-
"""data

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_eArAuS8ZA1O50gI_eoRsXZ-hdX7smBW

UPLOADING DATASETS INTO GOOGLE COLAB

To start, we need to import the relevant libraries and upload our datasets.
"""

!mkdir analysis #creates a folder to put all the datasets

import pandas as pd #imports a library so we can conduct statistical analysis 
from google.colab import files #allows us to import a file 
uploaded = files.upload()

import io
df1 = pd.read_csv(io.BytesIO(uploaded['popqualifications.csv'])) #giving the dataset a name

"""Now that the dataset has been imported, we can see its contents. This dataset shows the % of people without qualifications between 2015-2017, as well as the mean across the years. 

We will be using the **mean** since we need one value for each borough.


"""

df1

df1.describe()  #used to get summary statistics for a datset

df1.mean() #used to get averages for the dataset

clean1 = df1[['Borough','av 2015-2017']] #used to look at a particular part of the dataset 
clean1

import pandas as pd
from google.colab import files
uploaded = files.upload()

"""Now, we are uploading the dataset for income deprivation. Just like we did above, we can look at the dataset and will take the average values over the years. In this case, it's between 2015-2019 instead."""

import io
df2 = pd.read_csv(io.BytesIO(uploaded['incomedeprivation1.csv']))
df2

clean2 = df2[['Borough', 'av imd 2015-2019']]
clean2

import pandas as pd
from google.colab import files
uploaded = files.upload()

import io
df3 = pd.read_csv(io.BytesIO(uploaded['ethnicity1.csv']))
df3

import pandas as pd
from google.colab import files
uploaded = files.upload()

"""CAN WE CLEAN THE DATASET BELOW TO GET RID OF THE'UNNAMED' COLUMNS?"""

import io
df5 = pd.read_csv(io.BytesIO(uploaded['particpationdatasplit1.csv']))
df5

df_joined = df3.merge(df5, left_index=True, right_index=True) #decides what datasets to merge and where to merge the datasets
df_joined

df3_joined = df2.merge(df5, left_index=True, right_index=True) 
df3_joined

clean4 = df3_joined[['Used a public library service','av imd 2015-2019']] #getting a specific part of the dataset for analysis as there are repeats 
clean4

"""Scatter graph: the impact of deprivation on the use of free cultural infrastructure """

correlate = clean4.corr() #produces a correlation matrix 
correlate

clean4.describe()

clean4.mean()

clean4.kurtosis() #kurtosis

# Commented out IPython magic to ensure Python compatibility.
#Imported numerous libraries into Google Colab 
# %matplotlib inline 
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
import numpy as np

x = clean4['av imd 2015-2019'] #decides the x values 
y = clean4['Used a public library service'] #decides the y values 
plt.xlabel("Average index of multiple deprivation for London Boroughs 2015-2019") #axis label for x 
plt.ylabel("Used a public library service 2015-2017") #axis label for x 
plt.scatter(x,y); #plots scatterplot

from sklearn.linear_model import LinearRegression #imports statistical tool 
model = LinearRegression(fit_intercept=True)

model.fit(x[:, np.newaxis], y) #fits the linear model 

xfit = np.linspace(0,18,50) #sets the size 
yfit = model.predict(xfit[:, np.newaxis])

plt.xlabel("Average index of multiple deprivation for London Boroughs 2015-2019")
plt.ylabel("Used a public library service 2015-2017")

plt.scatter(x,y)
plt.plot(xfit, yfit);

print("Model slope: ", model.coef_[0]) #produces a numerical value of the coefficient for the linear regression 
print("Model intercept:", model.intercept_) #produces a numerical value of the intercept of the linear regression

"""Correlation matrix """

import pandas as pd
from google.colab import files
uploaded = files.upload()

import io
df20 = pd.read_csv(io.BytesIO(uploaded['heatmap.csv']))

df20
corr10 = df20.corr()
corr10

"""Heatmap"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline 
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
import numpy as np

mask = np.zeros_like(corr10, dtype=np.bool) #ignores the top triangle of the correlation matrix 
mask[np.triu_indices_from(mask)]= True

f, ax = plt.subplots(figsize=(15, 15))
heatmap = sns.heatmap(corr10,mask = mask, cmap="coolwarm", square = True, linewidths = .5, cbar_kws = {'shrink': .4}, vmin = -1, vmax = 1, annot = True, annot_kws = {'size': 15})

f, ax = plt.subplots(figsize=(15, 15)) #sets the figure size
heatmap = sns.heatmap(corr6,mask = mask,  #ignores any invalid values 
cmap=”coolwarm’ #sets the colour
 square = True,  #sets the shape 
 linewidths = .5,  #sets the width of the lines
cbar_kws = {'shrink': .4},  #sets the label size 
vmin = -1, vmax = 1, #sets the minimum and maximum colour bar value
annot = True, #sets the annotations for the values in the correlation matrix 
 annot_kws = {'size': 15}) #sets the annotation size